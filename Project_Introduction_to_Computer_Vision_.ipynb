{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOH9Hzt6JUcxkLEpRRPRWd3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataGuy-Kariuki/Data-Science-and-ML-BOOKS-/blob/main/Project_Introduction_to_Computer_Vision_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "\n",
        "## Business Context\n",
        "\n",
        "- Workplace safety in hazardous environments like construction sites and industrial plants is crucial to prevent accidents and injuries. One of the most important safety measures is ensuring workers wear safety helmets, which protect against head injuries from falling objects and machinery.\n",
        "- Non-compliance with helmet regulations increases the risk of serious injuries or fatalities, making effective monitoring essential, especially in large-scale operations where manual oversight is prone to errors and inefficiency.\n",
        "- To overcome these challenges, SafeGuard Corp plans to develop an automated image analysis system capable of detecting whether workers are wearing safety helmets. This system will improve safety enforcement, ensuring compliance and reducing the risk of head injuries.\n",
        "- By automating helmet monitoring, SafeGuard aims to enhance efficiency, scalability, and accuracy, ultimately fostering a safer work environment while minimizing human error in safety oversight.\n",
        "\n",
        "## Objective\n",
        "\n",
        "- As a data scientist at SafeGuard Corp, you are tasked with developing an image classification model that classifies images into one of two categories:\n",
        "- With Helmet: Workers wearing safety helmets.\n",
        "- Without Helmet: Workers not wearing safety helmets.\n",
        "\n",
        "\n",
        "## Data Description\n",
        "The dataset consists of 631 images, equally divided into two categories:\n",
        "\n",
        "- With Helmet: 311 images showing workers wearing helmets.\n",
        "- Without Helmet: 320 images showing workers not wearing helmets.\n",
        "\n",
        "## Dataset Characteristics:\n",
        "\n",
        "- Variations in Conditions: Images include diverse environments such as construction sites, factories, and industrial settings, with variations in lighting, angles, and worker postures to simulate real-world conditions.\n",
        "- Worker Activities: Workers are depicted in different actions such as standing, using tools, or moving, ensuring robust model learning for various scenarios."
      ],
      "metadata": {
        "id": "NhbhXXuKTOvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and Importing the Necessary Libraries"
      ],
      "metadata": {
        "id": "3x1nh1RvT5KI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgBaf5oVTAmu"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow[and-cuda] numpy==1.25.2 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note\n",
        "\n",
        "- After running the above cell, kindly restart the notebook kernel (for Jupyter Notebook) or runtime (for Google Colab) and run all cells sequentially from the next cell.\n",
        "\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook."
      ],
      "metadata": {
        "id": "8pSO6UerUthf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Tensorflow modules\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D,BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "# Display images using OpenCV\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "#Imports functions for evaluating the performance of machine learning models\n",
        "from sklearn.metrics import confusion_matrix, f1_score,accuracy_score, recall_score, precision_score, classification_report\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "vqB6jvVUUyqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed using keras.utils.set_random_seed. This will set:\n",
        "# 1) `numpy` seed\n",
        "# 2) backend random seed\n",
        "# 3) `python` random seed\n",
        "tf.keras.utils.set_random_seed(812)"
      ],
      "metadata": {
        "id": "p05j6qvEV-Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Overview\n",
        "\n",
        "  ## Loading the data"
      ],
      "metadata": {
        "id": "xra8PakgWCPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run the following code in case Google Colab is being used\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T2V2jtcqWGnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = np.load('/content/drive/MyDrive/task 7/images_proj.npy') #Complete the code to load the images\n",
        "\n",
        "labels = pd.read_csv('/content/drive/MyDrive/task 7/Labels_proj.csv') #Complete the code to load the labels"
      ],
      "metadata": {
        "id": "iKQxbusgWdca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape) #Complete the code to print the shape of the images\n",
        "print(labels.shape) #Complete the code to print the shape of the labels"
      ],
      "metadata": {
        "id": "vORYIShqWySq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "- Plot random images from each of the classes and print their corresponding labels."
      ],
      "metadata": {
        "id": "r54skrhSW-4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two lists of indices, one for each class\n",
        "helmet_indices = np.where(labels == 1)[0]\n",
        "no_helmet_indices = np.where(labels == 0)[0]\n",
        "\n",
        "# Select one image from each class\n",
        "helmet_img = images[np.random.choice(helmet_indices)]\n",
        "no_helmet_img = images[np.random.choice(no_helmet_indices)]\n",
        "\n",
        "# Plot the images\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "# Display \"With Helmet\" image\n",
        "axes[0].imshow(helmet_img)\n",
        "axes[0].set_title(\"Worker WITH Helmet\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Display \"Without Helmet\" image\n",
        "axes[1].imshow(no_helmet_img)\n",
        "axes[1].set_title(\"Worker WITHOUT Helmet\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IUr9V-hGXCMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking for class imbalance"
      ],
      "metadata": {
        "id": "LWYpSImcfFHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a count plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "ax = sns.countplot(x=labels.iloc[:, 0], palette=['red', 'green'])\n",
        "\n",
        "# Add exact counts on top of bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2, p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=10, )\n",
        "\n",
        "# Add labels\n",
        "plt.xlabel(\"Class Labels\", fontsize=12)\n",
        "plt.ylabel(\"Number of Images\", fontsize=12)\n",
        "plt.title(\"Count of Images per Class\", fontsize=14)\n",
        "plt.xticks(ticks=[0, 1], labels=[\"Without Helmet (0)\", \"With Helmet (1)\"])  # Rename x-axis labels\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KD2Pca1MfGIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The dataset is slightly imbalanced as we have 320 workers working without helmet and 311 workers using helment.\n",
        "- The difference between the two classes (311 vs. 320) is very small. This is not a major issue and is a good thing. A dataset is considered heavily imbalanced when one class has a significantly larger number of samples than the other (e.g., 90% of the data belongs to one class)."
      ],
      "metadata": {
        "id": "XuBzT2sNfjBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "### Converting images to grayscale"
      ],
      "metadata": {
        "id": "WdecVjjjf45o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_gray = []\n",
        "for i in range(len(images)):\n",
        "    img_gray = cv2.cvtColor(images[i], cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "    images_gray.append(img_gray)\n",
        "\n",
        "# Display a sample grayscale image\n",
        "n = 45 #Complete the code to define an index value\n",
        "cv2_imshow(images_gray[n])"
      ],
      "metadata": {
        "id": "U-TCRWO9f1iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the dataset"
      ],
      "metadata": {
        "id": "arqKJggCgmL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(np.array(images),labels , test_size=0.3, random_state=42,stratify=labels) #Complete the code to define the test_size\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp,y_temp , test_size=0.5, random_state=42,stratify=y_temp) #Complete the code to define the test_size"
      ],
      "metadata": {
        "id": "XEangBLWgnJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape,y_train.shape) #Complete the code to print the shape of the train data\n",
        "print(X_val.shape,y_val.shape) #Complete the code to print the shape of the validation data\n",
        "print(X_test.shape,y_test.shape) #Complete the code to print the shape of the test data"
      ],
      "metadata": {
        "id": "ggmYRGJc5I-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Normalization\n",
        "\n",
        "- Since the image pixel values range from 0-255, our method of normalization here will be scaling - we shall divide all the pixel values by 255 to standardize the images to have values between 0-1."
      ],
      "metadata": {
        "id": "0OO1K5XG5W3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_normalized = X_train.astype('float32')/255 #Complete the code to normalize the training images\n",
        "X_val_normalized = X_val.astype('float32')/255  #Complete the code to normalize the validation images\n",
        "X_test_normalized = X_test.astype('float32')/255   #Complete the code to normalize the test images"
      ],
      "metadata": {
        "id": "98Pa_Yup5ZlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building\n",
        "\n",
        "### Utility Functions"
      ],
      "metadata": {
        "id": "LPrGJHL_5qlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using statsmodels\n",
        "def model_performance_classification(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # checking which probabilities are greater than threshold\n",
        "    pred = model.predict(predictors).reshape(-1)>0.5\n",
        "\n",
        "    target = target.to_numpy().reshape(-1)\n",
        "\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred, average='weighted')  # to compute Recall\n",
        "    precision = precision_score(target, pred, average='weighted')  # to compute Precision\n",
        "    f1 = f1_score(target, pred, average='weighted')  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame({\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1 Score\": f1,},index=[0],)\n",
        "\n",
        "    return df_perf"
      ],
      "metadata": {
        "id": "EIRB5ZR75tdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(model,predictors,target,ml=False):\n",
        "    \"\"\"\n",
        "    Function to plot the confusion matrix\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    ml: To specify if the model used is an sklearn ML model or not (True means ML model)\n",
        "    \"\"\"\n",
        "\n",
        "    # checking which probabilities are greater than threshold\n",
        "    pred = model.predict(predictors).reshape(-1)>0.5\n",
        "\n",
        "    target = target.to_numpy().reshape(-1)\n",
        "\n",
        "    # Plotting the Confusion Matrix using confusion matrix() function which is also predefined tensorflow module\n",
        "    confusion_matrix = tf.math.confusion_matrix(target,pred)\n",
        "    f, ax = plt.subplots(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        confusion_matrix,\n",
        "        annot=True,\n",
        "        linewidths=.4,\n",
        "        fmt=\"d\",\n",
        "        square=True,\n",
        "        ax=ax\n",
        "    )\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "b5LNNF9q59ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Simple Convolutional Neural Network (CNN)\n",
        "- Let's build a CNN Model.\n",
        "\n",
        "#### The model has 2 main parts:\n",
        "\n",
        "- The Feature Extraction layers which are comprised of convolutional and pooling layers.\n",
        "- The Fully Connected classification layers for prediction.\n"
      ],
      "metadata": {
        "id": "6MYZiMO_6Hax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing Model\n",
        "model_1 = Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model_1.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\", input_shape=(200,200,3))) #Complete the code to define the shape of the input image\n",
        "model_1.add(MaxPooling2D((4, 4), padding='same'))\n",
        "model_1.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\")) #Complete the code to define the number of output channels,the kernel shape and the activation function\n",
        "model_1.add(MaxPooling2D((2,2), padding='same')) #Complete the code to define the shape of the pooling kernel\n",
        "model_1.add(Conv2D(128, (3,3), activation='relu', padding=\"same\")) #Complete the code to define the number of output channels,the kernel shape and the activation function\n",
        "\n",
        "# Flatten and Dense layers\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(64, activation='relu'))\n",
        "model_1.add(Dense(1, activation='sigmoid'))  #Complete the code to define the number of neurons in the output layer and the activation function\n",
        "\n",
        "# Compile with Adam Optimizer\n",
        "opt = Adam(learning_rate=0.001) #Complete the code to define the learning rate.\n",
        "model_1.compile(optimizer=opt, loss='binary_crossentropy', metrics=[\"accuracy\", \"precision\", \"recall\"]) #Complete the code to define the metric of choice from Precision,f1_score,Recall\n",
        "\n",
        "# Summary\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "Ru9qfc7t6RpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_1 = model_1.fit(\n",
        "            X_train_normalized, y_train,\n",
        "            epochs=20, #Complete the code to define the number of epochs\n",
        "            validation_data=(X_val_normalized,y_val),\n",
        "            shuffle=True,\n",
        "            batch_size=32, #Complete the code to define the batch size\n",
        "            verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "ufDoPNzI8KaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_1.history['accuracy']) #Complete the code to plot the train metrics\n",
        "plt.plot(history_1.history['val_accuracy']) #Complete the code to plot the validation data metrics\n",
        "plt.title('Model Accuracy') #Complete the code to define the title for the plot\n",
        "plt.ylabel('Accuracy') #Complete the code to define the label for the y-axis\n",
        "plt.xlabel('Epoch') #Complete the code to define the label for the x-axis\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fXxcCrMe8n8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_train_perf = model_performance_classification(model_1, X_train_normalized,y_train)\n",
        "\n",
        "print(\"Train performance metrics\")\n",
        "print(model_1_train_perf)"
      ],
      "metadata": {
        "id": "gdjcCcjy8wI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Accuracy (1.0): The model correctly classified every single image in the training set.\n",
        "- Recall (1.0): The model correctly identified all the positive cases (workers with helmets) in the training set. There were no false negatives.\n",
        "- Precision (1.0): When the model predicted a positive case (worker with a helmet), it was always correct. There were no false positives.\n",
        "- F1 Score (1.0): This is the harmonic mean of precision and recall, and a score of 1.0 indicates a perfect balance between the two."
      ],
      "metadata": {
        "id": "wtS43poP-zxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_1,X_train_normalized,y_train)"
      ],
      "metadata": {
        "id": "gb_jwmU_82fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_valid_perf = model_performance_classification(model_1, X_val_normalized,y_val)\n",
        "\n",
        "print(\"Validation performance metrics\")\n",
        "print(model_1_valid_perf)"
      ],
      "metadata": {
        "id": "gEvJ3IFL875-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Accuracy (0.989474): This means that approximately 98.94% of the images in the validation set were correctly classified by the model. This is a very high accuracy, suggesting the model is generally good at distinguishing between images with and without helmets.\n",
        "- Recall (0.989474): This indicates that the model correctly identified about 98.94% of the actual positive cases (workers with helmets) in the validation set. This is important for minimizing false negatives (missing a worker who is not wearing a helmet).\n",
        "- Precision (0.989683): This means that when the model predicted a positive case (worker with a helmet), it was correct about 98.96% of the time. This is important for minimizing false positives (incorrectly flagging a worker as not wearing a helmet when they are).\n",
        "-  The F1 score is a single metric that balances both precision and recall. An F1 score of 0.989474 means that your model has a very good balance between correctly identifying positive cases (recall) and not incorrectly labeling negative cases as positive (precision) on the validation data."
      ],
      "metadata": {
        "id": "uwYziAEj_TXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_1,X_val_normalized,y_val)"
      ],
      "metadata": {
        "id": "X66E5xJB9BdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vizualizing the predictions"
      ],
      "metadata": {
        "id": "XWQm91sF9G-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For index 12 (corrected)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_val[12])\n",
        "plt.show()\n",
        "prediction = model_1.predict(X_val_normalized[12].reshape(1,200,200,3))  # Changed to 12\n",
        "predicted_label = prediction[0][0]>0.5\n",
        "print('Predicted Label:', 1 if predicted_label else 0)\n",
        "true_label = y_val.iloc[12]\n",
        "print('True Label:', true_label)\n",
        "\n",
        "# For index 33 (corrected)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_val[33])\n",
        "plt.show()\n",
        "prediction = model_1.predict(X_val_normalized[33].reshape(1,200,200,3))  # Changed to 33\n",
        "predicted_label = prediction[0][0]>0.5\n",
        "print('Predicted Label:', 1 if predicted_label else 0)\n",
        "true_label = y_val.iloc[33]\n",
        "print('True Label:', true_label)"
      ],
      "metadata": {
        "id": "s9IxxdRSBijb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step:\n",
        "- Processed 1 image in 30 milliseconds\n",
        "\n",
        "Predicted Label: 1:\n",
        "- Your model predicted \"With Helmet\" ✅\n",
        "\n",
        "True Label: Label 1:\n",
        "- The actual label is \"With Helmet\" ✅\n",
        "\n",
        "This image is from row 173 in your validation set"
      ],
      "metadata": {
        "id": "a0m9doScCDO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: (VGG-16 (Base))\n",
        "\n",
        "- We will be loading a pre-built architecture - VGG16, which was trained on the ImageNet dataset and is the runner-up in the ImageNet competition in 2014.\n",
        "\n",
        "- or training VGG16, we will directly use the convolutional and pooling layers and freeze their weights i.e. no training will be done on them. For classification, we will add a Flatten and a single dense layer."
      ],
      "metadata": {
        "id": "nQDxrMRUCQbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3)) #Complete the code to define the shape of the image\n",
        "vgg_model.summary()"
      ],
      "metadata": {
        "id": "xLWDqr_HCH9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making all the layers of the VGG model non-trainable. i.e. freezing them\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "0TPxWZntCoOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = Sequential()\n",
        "\n",
        "# Adding the convolutional part of the VGG16 model from above\n",
        "model_2.add(vgg_model)\n",
        "\n",
        "# Flattening the output of the VGG16 model because it is from a convolutional layer\n",
        "model_2.add(Flatten())\n",
        "\n",
        "# Adding a dense output layer\n",
        "model_2.add(Dense(1, activation='sigmoid')) #Complete the code to define the number of neurons in the output layer."
      ],
      "metadata": {
        "id": "ggvtV6EBCz8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.001) #Complete the code to define the learning rate\n",
        "# Compile model\n",
        "model_2.compile(optimizer=opt, loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\", \"precision\", \"recall\"]) #Complete the code to define the metrics"
      ],
      "metadata": {
        "id": "dHP9WSi_C9_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the summary of the model\n",
        "model_2.summary()"
      ],
      "metadata": {
        "id": "lK0-UsXxDEN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator()"
      ],
      "metadata": {
        "id": "oq5-G8YNDJCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs\n",
        "epochs = 10 #Complete the code to define the epochs\n",
        "# Batch size\n",
        "batch_size = 32 #Complete the code to define the batch size\n",
        "\n",
        "history_2 = model_2.fit(train_datagen.flow(X_train_normalized,y_train,\n",
        "                                      batch_size=batch_size,\n",
        "                                      seed=42,\n",
        "                                      shuffle=False),\n",
        "                    epochs=epochs,\n",
        "                    steps_per_epoch=X_train_normalized.shape[0] // batch_size,\n",
        "                    validation_data=(X_val_normalized,y_val),\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "IFtUyAeMDXku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_2.history['accuracy']) #Complete the code to plot the train metrics\n",
        "plt.plot(history_2.history['val_accuracy']) #Complete the code to plot the validation data metrics\n",
        "plt.title('Model 2 Accuracy') #Complete the code to define the title for the plot\n",
        "plt.ylabel('Accuracy') #Complete the code to define the label for the y-axis\n",
        "plt.xlabel('Epoch') #Complete the code to define the label for the x-axis\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Oeol6ZNADpcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_train_perf = model_performance_classification(model_2,X_train_normalized,y_train)\n",
        "\n",
        "print(\"Train performance metrics\")\n",
        "print(model_2_train_perf)"
      ],
      "metadata": {
        "id": "KocF54cBDvkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_2,X_train_normalized,y_train)"
      ],
      "metadata": {
        "id": "tJyxqIWeDzyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_valid_perf = model_performance_classification(model_2, X_val_normalized,y_val)\n",
        "\n",
        "print(\"Validation performance metrics\")\n",
        "print(model_2_valid_perf)"
      ],
      "metadata": {
        "id": "CUFYlD1HD4MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_2,X_val_normalized,y_val)"
      ],
      "metadata": {
        "id": "nvZiHESlD8J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the prediction:"
      ],
      "metadata": {
        "id": "NFju9-2KEFrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For first prediction (using index 0 as example)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_val[20]) #Complete the code to define the index\n",
        "plt.show()\n",
        "prediction = model_2.predict(X_val_normalized[20].reshape(1,200,200,3)) #Complete the code to define the index\n",
        "predicted_label = prediction[0][0]>0.5  # Extract the predicted class label\n",
        "print('Predicted Label:', 1 if predicted_label else 0)\n",
        "# Fix indexing issue in y_val\n",
        "true_label = y_val.iloc[20] #Complete the code to define the index\n",
        "print('True Label:', true_label)\n",
        "\n",
        "# For second prediction (using index 1 as example)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_val[11]) #Complete the code to define the index\n",
        "plt.show()\n",
        "prediction = model_2.predict(X_val_normalized[11].reshape(1,200,200,3)) #Complete the code to define the index\n",
        "predicted_label = prediction[0][0]>0.5  # Extract the predicted class label\n",
        "print('Predicted Label:', 1 if predicted_label else 0)\n",
        "# Fix indexing issue in y_val\n",
        "true_label = y_val.iloc[11] #Complete the code to define the index\n",
        "print('True Label:', true_label)"
      ],
      "metadata": {
        "id": "wuJI-EeREG3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: (VGG-16 (Base + FFNN))\n",
        "\n",
        "- We will directly use the convolutional and pooling layers and freeze their weights i.e. no training will be done on them. For classification, we will add a Flatten layer and a Feed Forward Neural Network."
      ],
      "metadata": {
        "id": "_-RlZMR9EVCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = Sequential()\n",
        "\n",
        "# Adding the convolutional part of the VGG16 model from above\n",
        "model_3.add(vgg_model)\n",
        "\n",
        "# Flattening the output of the VGG16 model because it is from a convolutional layer\n",
        "model_3.add(Flatten())\n",
        "\n",
        "# Adding the Feed Forward neural network\n",
        "model_3.add(Dense(256, activation='relu')) #Complete the code to define the number of neurons and the activation function\n",
        "model_3.add(Dropout(rate=0.5)) #Complete the code to define the dropout rate\n",
        "model_3.add(Dense(128, activation='relu')) #Complete the code to define the number of neurons and the activation function\n",
        "\n",
        "# Adding a dense output layer\n",
        "model_3.add(Dense(1, activation='sigmoid')) #Complete the code to define the number of neurons in the output layer and the activation function"
      ],
      "metadata": {
        "id": "vvfsvH9CEXcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.001) #Complete the code to define the learning rate"
      ],
      "metadata": {
        "id": "lHMP18B3Ep-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model_3.compile(optimizer=opt, loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\", \"precision\", \"recall\"]) #Complete the code to define the metrics"
      ],
      "metadata": {
        "id": "jmCH94QqEw_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the summary of the model\n",
        "model_3.summary()"
      ],
      "metadata": {
        "id": "oEQt46UsEywZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_3 = model_3.fit(train_datagen.flow(X_train_normalized,y_train,\n",
        "                                       batch_size=32, #Complete the code to define the batch size\n",
        "                                       seed=42,\n",
        "                                       shuffle=False),\n",
        "                    epochs=15, #Complete the code to define the number of epochs\n",
        "                    steps_per_epoch=X_train_normalized.shape[0] // batch_size,\n",
        "                    validation_data=(X_val_normalized,y_val),\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "Ar1Y0o4PFIYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_3.history['accuracy']) #Complete the code to plot the train metrics\n",
        "plt.plot(history_3.history['val_accuracy']) #Complete the code to plot the validation data metrics\n",
        "plt.title('Model 3 Accuracy') #Complete the code to define the title for the plot\n",
        "plt.ylabel('Accuracy') #Complete the code to define the label for the y-axis\n",
        "plt.xlabel('Epoch') #Complete the code to define the label for the x-axis\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YcdvDvMQFUNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_train_perf = model_performance_classification(model_3, X_train_normalized,y_train)\n",
        "\n",
        "print(\"Train performance metrics\")\n",
        "print(model_3_train_perf)"
      ],
      "metadata": {
        "id": "g3nFBKjSFZIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_3,X_train_normalized,y_train)"
      ],
      "metadata": {
        "id": "Ow2VfQNTFb4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_valid_perf = model_performance_classification(model_3, X_val_normalized,y_val)\n",
        "\n",
        "print(\"Validation performance metrics\")\n",
        "print(model_3_valid_perf)"
      ],
      "metadata": {
        "id": "DdVwNiLfFfna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_3,X_val_normalized,y_val)"
      ],
      "metadata": {
        "id": "kcY_fvHvFkZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the predictions"
      ],
      "metadata": {
        "id": "6DrFUAJlFoa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For first prediction (using index 0 as example)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_val[0]) #Complete the code to define the index\n",
        "plt.show()\n",
        "prediction = model_3.predict(X_val_normalized[0].reshape(1,200,200,3)) #Complete the code to define the index\n",
        "predicted_label = prediction[0][0]>0.5  # Extract the predicted class label\n",
        "print('Predicted Label:', 1 if predicted_label else 0)\n",
        "# Fix indexing issue in y_val\n",
        "true_label = y_val.iloc[0] #Complete the code to define the index\n",
        "print('True Label:', true_label)\n",
        "\n",
        "# For second prediction (using index 1 as example)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_val[15]) #Complete the code to define the index\n",
        "plt.show()\n",
        "prediction = model_3.predict(X_val_normalized[15].reshape(1,200,200,3)) #Complete the code to define the index\n",
        "predicted_label = prediction[0][0]>0.5  # Extract the predicted class label\n",
        "print('Predicted Label:', 1 if predicted_label else 0)\n",
        "# Fix indexing issue in y_val\n",
        "true_label = y_val.iloc[15] #Complete the code to define the index\n",
        "print('True Label:', true_label)"
      ],
      "metadata": {
        "id": "kROE9zrhFppQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4: (VGG-16 (Base + FFNN + Data Augmentation)\n",
        "\n",
        "In most of the real-world case studies, it is challenging to acquire a large number of images and then train CNNs.\n",
        "\n",
        "To overcome this problem, one approach we might consider is Data Augmentation.\n",
        "\n",
        "CNNs have the property of translational invariance, which means they can recognise an object even if its appearance shifts translationally in some way. - Taking this attribute into account, we can augment the images using the techniques listed below\n",
        "\n",
        "- Horizontal Flip (should be set to True/False)\n",
        "- Vertical Flip (should be set to True/False)\n",
        "- Height Shift (should be between 0 and 1)\n",
        "- Width Shift (should be between 0 and 1)\n",
        "- Rotation (should be between 0 and 180)\n",
        "- Shear (should be between 0 and 1)\n",
        "- Zoom (should be between 0 and 1) etc.\n",
        "\n",
        "Remember, data augmentation should not be used in the validation/test data set."
      ],
      "metadata": {
        "id": "ue2xeGQaGoMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_4 = Sequential()\n",
        "\n",
        "# Adding the convolutional part of the VGG16 model from above\n",
        "model_4.add(vgg_model)\n",
        "\n",
        "# Flattening the output of the VGG16 model because it is from a convolutional layer\n",
        "model_4.add(Flatten())\n",
        "\n",
        "# Adding the Feed Forward neural network\n",
        "model_4.add(Dense(256, activation='relu')) #Complete the code to define the number of neurons and the activation function\n",
        "model_4.add(Dropout(rate=0.5)) #Complete the code to define the dropout rate\n",
        "model_4.add(Dense(128, activation='relu')) #Complete the code to define the number of neurons and the activation function\n",
        "\n",
        "# Adding a dense output layer\n",
        "model_4.add(Dense(1, activation='sigmoid')) #Complete the code to define the number of neurons in the output layer and the activation function"
      ],
      "metadata": {
        "id": "nhly7ockGzwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.001)\n",
        "# Compile model\n",
        "model_4.compile(optimizer=opt, loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"]) #Complete the code to define the metrics"
      ],
      "metadata": {
        "id": "QbJxcUBfHbQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the summary of the model\n",
        "model_4.summary()"
      ],
      "metadata": {
        "id": "0bRv7N5WHfAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,           #Complete the code to define the range for rotation\n",
        "    fill_mode='nearest',\n",
        "    width_shift_range=0.2,       #Complete the code to define the parameters for the data augmentation\n",
        "    height_shift_range=0.2,      #Complete the code to define the parameters for the data augmentation\n",
        "    shear_range=0.2,             #Complete the code to define the parameters for the data augmentation\n",
        "    zoom_range=0.2               #Complete the code to define the parameters for the data augmentation\n",
        ")"
      ],
      "metadata": {
        "id": "4LGmI-twHo0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_4 = model_4.fit(train_datagen.flow(X_train_normalized,y_train,\n",
        "                                       batch_size=32, #Complete the code to define the batch size\n",
        "                                       seed=42,\n",
        "                                       shuffle=False),\n",
        "                    epochs=15, #Assuming epochs is defined earlier, otherwise use a number like 15\n",
        "                    steps_per_epoch=X_train_normalized.shape[0] // batch_size,\n",
        "                    validation_data=(X_val_normalized,y_val),\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "ZFbie7O6Hvce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_4.history['accuracy']) #Complete the code to plot the train metrics\n",
        "plt.plot(history_4.history['val_accuracy']) #Complete the code to plot the validation data metrics\n",
        "plt.title('Model 4 Accuracy (With Data Augmentation)') #Complete the code to define the title for the plot\n",
        "plt.ylabel('Accuracy') #Complete the code to define the label for the y-axis\n",
        "plt.xlabel('Epoch') #Complete the code to define the label for the x-axis\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uh-vTR-VH8Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_train_perf = model_performance_classification(model_4, X_train_normalized,y_train)\n",
        "\n",
        "print(\"Train performance metrics\")\n",
        "print(model_4_train_perf)"
      ],
      "metadata": {
        "id": "EUBL4UL5H_i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_4,X_train_normalized,y_train)"
      ],
      "metadata": {
        "id": "t1hkmduVIKZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_valid_perf = model_performance_classification(model_4, X_val_normalized,y_val)\n",
        "\n",
        "print(\"Validation performance metrics\")\n",
        "print(model_4_valid_perf)"
      ],
      "metadata": {
        "id": "TXeDdbv7IN6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_4,X_val_normalized,y_val)"
      ],
      "metadata": {
        "id": "WqJPPdmmIRZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the predictions"
      ],
      "metadata": {
        "id": "RTUfhXLvIYx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For first prediction (using index 0 as example)\n",
        "# For first prediction (using index 10)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_val[10]) #Complete the code to define the index\n",
        "plt.show()\n",
        "prediction = model_4.predict(X_val_normalized[10].reshape(1,200,200,3)) #Complete the code to define the index\n",
        "predicted_label = prediction[0][0]>0.5  # Extract the predicted class label (FIXED: use [0][0] not [12][0])\n",
        "print('Predicted Label:', 1 if predicted_label else 0)\n",
        "# Fix indexing issue in y_val\n",
        "true_label = y_val.iloc[10] #Complete the code to define the index\n",
        "print('True Label:', true_label)\n",
        "\n",
        "# For second prediction (using index 7)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_val[7]) #Complete the code to define the index\n",
        "plt.show()\n",
        "prediction = model_4.predict(X_val_normalized[7].reshape(1,200,200,3)) #Complete the code to define the index\n",
        "predicted_label = prediction[0][0]>0.5  # Extract the predicted class label\n",
        "print('Predicted Label:', 1 if predicted_label else 0)\n",
        "# Fix indexing issue in y_val\n",
        "true_label = y_val.iloc[7] #Complete the code to define the index\n",
        "print('True Label:', true_label)"
      ],
      "metadata": {
        "id": "-wR1Bq9LIeTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance Comparison and Final Model Selection"
      ],
      "metadata": {
        "id": "pacC9lTYM7z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        model_1_train_perf.T,\n",
        "        model_2_train_perf.T,\n",
        "        model_3_train_perf.T,\n",
        "        model_4_train_perf.T,\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    \"Simple Convolutional Neural Network (CNN)\",\"VGG-16 (Base)\",\"VGG-16 (Base+FFNN)\",\"VGG-16 (Base+FFNN+Data Aug)\"\n",
        "]"
      ],
      "metadata": {
        "id": "FMkUVbF_M863"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_valid_comp_df = pd.concat(\n",
        "    [\n",
        "        model_1_valid_perf.T,\n",
        "        model_2_valid_perf.T,\n",
        "        model_3_valid_perf.T,\n",
        "        model_4_valid_perf.T\n",
        "\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_valid_comp_df.columns = [\n",
        " \"Simple Convolutional Neural Network (CNN)\",\"VGG-16 (Base)\",\"VGG-16 (Base+FFNN)\",\"VGG-16 (Base+FFNN+Data Aug)\"\n",
        "]"
      ],
      "metadata": {
        "id": "PiDAdpYtNDsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_train_comp_df"
      ],
      "metadata": {
        "id": "68TyL6-iNG2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_valid_comp_df"
      ],
      "metadata": {
        "id": "I4BNcfaSNHcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_train_comp_df - models_valid_comp_df"
      ],
      "metadata": {
        "id": "mjy6QeyINO1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Performance"
      ],
      "metadata": {
        "id": "pQFh1yewNTZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_test_perf = model_performance_classification(model_4, X_test_normalized, y_test) #Complete the code to pass the best model"
      ],
      "metadata": {
        "id": "Tq9uQVRMNUMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typical Performance Hierarchy:\n",
        "- Model 4: VGG-16 + FFNN + Augmentation (Best)\n",
        "- Model 3: VGG-16 + FFNN (Very Good)\n",
        "- Model 2: VGG-16 Base (Good)\n",
        "- Model 1: Simple CNN (Baseline)\n",
        "\n",
        "The data augmentation in Model 4 is what usually makes it outperform Model 3, as it effectively gives you a much larger and more diverse training set without collecting more real images."
      ],
      "metadata": {
        "id": "nNdDOpXXOGDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_test_perf"
      ],
      "metadata": {
        "id": "bQv6q2E8N7R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_4, X_test_normalized, y_test) #Complete the code to pass the best model"
      ],
      "metadata": {
        "id": "WWt8Q5G7OQEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actionable Insights & Recommendations"
      ],
      "metadata": {
        "id": "9LdR6sY9OU6U"
      }
    }
  ]
}